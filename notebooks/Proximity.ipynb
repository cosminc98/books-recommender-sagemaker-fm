{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759aea6f-399f-46b9-9a1b-4002a6b26484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left; display:block}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left; display:block}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68852e1-e451-434e-bd2c-93b79b49100e",
   "metadata": {},
   "source": [
    "# Proximity Book Recommendations\n",
    "---\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "    * [Background](#Introduction-Background)\n",
    "    * [Definitions](#Introduction-Definitions)\n",
    "    * [Prerequisites](#Introduction-Prerequisites)\n",
    "2. [Notebook Setup](#Notebook-Setup)\n",
    "3. [Code](#Code)\n",
    "    * [Imports](#Code-Imports)\n",
    "    * [File Paths](#Code-File-Paths)\n",
    "    * [Load Input Files](#Code-Load-Input-Files)\n",
    "    * [Proximity Search Features](#Code-Proximity-Search-Features)\n",
    "    * [Proximity Search Algorithm](#Code-Proximity-Search-Algorithm)\n",
    "    * [Parallel Proximity Search](#Code-Parallel-Proximity-Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d544e2e-7a84-414d-ae95-b723f261c315",
   "metadata": {},
   "source": [
    "<a id=\"Introduction\"></a>\n",
    "## Introduction\n",
    "---\n",
    "<a id=\"Introduction-Background\"></a>\n",
    "### Background\n",
    "In our application we'll have anonymous users provide a few books that they liked and we need to give them book recommendations based on those. Our factorization machine model provides a way to score books given the user context (the books that the user said they liked), but there are almost 200k to search from, which would be computationally expensive to score them all and may introduce noise from wrong predictions. This notebook creates a way to vastly reduce the search space for book recommendation. We will find around a fixed number of proximal books for each book from the context subset (see [definitions](#Introduction-Definitions)). A dictionary of the mapping is saved in the following json format:\n",
    "```json\n",
    "{\n",
    "    \"ISBN_0\": [\"ISBN_4\", \"ISBN_5\", ...],\n",
    "    \"ISBN_1\": [\"ISBN_6\", \"ISBN_7\", ...],\n",
    "    \"ISBN_2\": [\"ISBN_8\", \"ISBN_9\", ...],\n",
    "    ... \n",
    "}\n",
    "```\n",
    "The file above will be named \"isbn_to_proximal_isbns.json\". In the example, the keys (ISBN 0 through 2) must be from the context subset, and the values (ISBN 4 through 9) must be from the target subset (see [definitions](#Introduction-Definitions) to understand why).\n",
    "Using this dictionary we can quickly find a few books that are closely related to those that the user liked and we can put them in order using the factorization machine model. If we want to generate even more recommendations, all we need to do is iteratively score the first proximal recommendations and get new proximal recommandations from the books with the largest score.\n",
    "\n",
    "<a id=\"Introduction-Definitions\"></a>\n",
    "### Definitions\n",
    "| Term | Definition |\n",
    "|:--- |:--- | \n",
    "| Context Book | A book that is provided by the user, with the assumption that they liked it (explicit feedback), that will be used to rank recommendations using the factorization machine model. The context books are only a subset of the book dataset since some books did not have enough data during training. **Users may only choose liked books from this subset.** |\n",
    "| Target Book | A book that can be scored by the factorization machine model. The target books are only a subset of the book dataset since some books did not have enough data during training. **Users will receive recommendations only from this subset.** |\n",
    "| Encoder | The encoders we refer to in this notebook are either **sklearn.preprocessing.OneHotEncoder** for the target books (since we only have one per prediction), or **sklearn.preprocessing.MultiLabelBinarizer** for the context books (since we have multiple ones for each prediction). **We load these encoders to retrieve the context and target book subsets.** |\n",
    "| Proximal Book | A context book has multiple proximal books that are \"close\" to it. Here, \"close\" means that users that liked the context book also liked the proximal books. Books from the same authors are also considered proximal. |\n",
    "| ISBN | International Standard Book Numbers (or ISBN) is a unique identified for each book. In particular, it is an identified for each specific version/revision of a given book, which is why we need to perform ISBN deduplication to map old versions to the latest one. We do this to treat all versions of the same book in as one single book. |\n",
    "\n",
    "<a id=\"Introduction-Prerequisites\"></a>\n",
    "### Prerequisites\n",
    "The following files are required, all of which are the result of training the model using the training notebook:\n",
    "- target_encoder.pkl\n",
    "- context_encoder.pkl\n",
    "- same_book_isbn_map.json\n",
    "- valid_isbn.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807a753-eb1c-4b31-af02-6cd5e05bb315",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"Notebook-Setup\"></a>\n",
    "## Notebook Setup\n",
    "---\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.c5.2xlarge (8 vCPUs, 16GB RAM) instance with Python 3 (Data Science) kernel. \n",
    "It may also work with smaller instances, but at least 8GB of RAM is recommended. With ml.c5.2xlarge, the processing time is around 35 minutes.\n",
    "You may use instances with even more CPUs, but make sure to close them immediately after the processing is done and the dictionary is saved as a .json file to not be charged a large sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407f643-28e8-409f-afd5-2e08f119ca68",
   "metadata": {},
   "source": [
    "<a id=\"Code\"></a>\n",
    "## Code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70d99a-e008-406f-b35b-ff6a7810a9dc",
   "metadata": {},
   "source": [
    "<a id=\"Code-Imports\"></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92e9027-59bb-4310-af7d-5e14c13e8af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Set, Callable\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54055a69-0f5f-4165-82e4-840a00cc0c09",
   "metadata": {},
   "source": [
    "<a id=\"Code-File-Paths\"></a>\n",
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc418c7e-c14f-48d4-a6da-3932439216fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input paths\n",
    "model_dir = \"./\"\n",
    "valid_isbns_fpath = os.path.join(model_dir, \"valid_isbn.json\")\n",
    "same_book_isbn_fpath = os.path.join(model_dir, \"same_book_isbn_map.json\")\n",
    "target_encoder_fpath = os.path.join(model_dir, \"target_encoder.pkl\")\n",
    "context_encoder_fpath = os.path.join(model_dir, \"context_encoder.pkl\")\n",
    "\n",
    "data_dir = \"../../../data\"\n",
    "ratings_fpath = os.path.join(data_dir, \"books\", \"ratings-train.csv\")\n",
    "books_fpath = os.path.join(data_dir, \"books\", \"books.csv\")\n",
    "\n",
    "# output paths\n",
    "output_dir = \"./\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_fpath = os.path.join(output_dir, \"isbn_to_proximal_isbns.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af4719-b5ff-48ca-995a-b80d1a055c63",
   "metadata": {},
   "source": [
    "<a id=\"Code-Load-Input-Files\"></a>\n",
    "### Load Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3797fc2f-79e2-4b3f-9ad3-9a4228f24288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 149623 target books\n"
     ]
    }
   ],
   "source": [
    "with open(target_encoder_fpath, \"rb\") as f:\n",
    "    target_encoder = pickle.load(f)\n",
    "target_isbns = set(target_encoder.categories_[0])\n",
    "print(f\"There are {len(target_isbns)} target books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c044c4fd-bc5c-479a-aea2-7bc53692c8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 170978 context books\n"
     ]
    }
   ],
   "source": [
    "with open(context_encoder_fpath, \"rb\") as f:\n",
    "    context_encoder = pickle.load(f)\n",
    "context_isbns = set(context_encoder.classes_)\n",
    "print(f\"There are {len(context_isbns)} context books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eed9476-ab6f-4add-97b5-ea8e55d647b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 263516 total valid ISBNs (books)\n"
     ]
    }
   ],
   "source": [
    "with open(valid_isbns_fpath, \"r\") as f:\n",
    "    valid_isbns = set(json.load(f))\n",
    "print(f\"There are {len(valid_isbns)} total valid ISBNs (books)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7084b328-0cfa-45f9-b73d-890a717295c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24636 duplicate ISBNs (different versions of the same book) that need to be mapped to the latest version of that book (ISBN)\n"
     ]
    }
   ],
   "source": [
    "with open(same_book_isbn_fpath, \"r\") as f:\n",
    "    same_book_isbn_map = json.load(f)\n",
    "print(f\"There are {len(same_book_isbn_map)} duplicate ISBNs (different versions of the same book) that need to be mapped to the latest version of that book (ISBN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4dadaa-4eec-4e15-ba43-b1c35a48823e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>BookRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25409</td>\n",
       "      <td>081296666X</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25533</td>\n",
       "      <td>0440910846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26182</td>\n",
       "      <td>1570625190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26624</td>\n",
       "      <td>0698119517</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26731</td>\n",
       "      <td>0515087947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034894</th>\n",
       "      <td>274308</td>\n",
       "      <td>0671543032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034895</th>\n",
       "      <td>275154</td>\n",
       "      <td>0375703063</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034897</th>\n",
       "      <td>275970</td>\n",
       "      <td>0553348973</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034898</th>\n",
       "      <td>275970</td>\n",
       "      <td>0850253101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034899</th>\n",
       "      <td>275970</td>\n",
       "      <td>1585422762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904559 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID        ISBN  BookRating\n",
       "0         25409  081296666X          10\n",
       "1         25533  0440910846           0\n",
       "2         26182  1570625190           0\n",
       "3         26624  0698119517           0\n",
       "4         26731  0515087947           0\n",
       "...         ...         ...         ...\n",
       "1034894  274308  0671543032           0\n",
       "1034895  275154  0375703063           8\n",
       "1034897  275970  0553348973          10\n",
       "1034898  275970  0850253101           0\n",
       "1034899  275970  1585422762           0\n",
       "\n",
       "[904559 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_if_duplicate(isbn: str, same_book_isbn_map: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Map current ISBN to the latest version if it is a duplicate.\n",
    "    \n",
    "    Args:\n",
    "        isbn: The ISBN to be mapped if it is a duplicate.\n",
    "        same_book_isbn_map: A map from duplicate ISBN's (old version of a book)\n",
    "            to the latest ISBN (the latest version of that book).\n",
    "            \n",
    "    Returns:\n",
    "        The ISBN after being mapped or not.\n",
    "    \"\"\"\n",
    "    if isbn in same_book_isbn_map:\n",
    "        return same_book_isbn_map[isbn]\n",
    "    return isbn\n",
    "\n",
    "\n",
    "def read_user_ratings(\n",
    "    file_path: str, \n",
    "    same_book_isbn_map: Dict[str, str], \n",
    "    valid_isbns: Set[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the user ratings, remove ratings with invalid ISBNs, and map \n",
    "    duplicates to the latest version.\n",
    "    \n",
    "    Args:\n",
    "        file_path: File path of the user ratings file.\n",
    "        same_book_isbn_map: A map from duplicate ISBN's (old version of a book)\n",
    "            to the latest ISBN (the latest version of that book).\n",
    "        valid_isbns: Set of valid ISBNs.\n",
    "        \n",
    "    Returns:\n",
    "        The user ratings dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, dtype={\"UserID\": str, \"ISBN\": str, \"BookRating\": int})\n",
    "    df = df[df.ISBN.isin(valid_isbns)]\n",
    "    df[\"ISBN\"] = df[\"ISBN\"].apply(partial(map_if_duplicate, same_book_isbn_map=same_book_isbn_map))\n",
    "    df.drop_duplicates(subset=[\"UserID\", \"ISBN\"], keep=\"last\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "ratings_df = read_user_ratings(ratings_fpath, same_book_isbn_map, valid_isbns)\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efff087-a12d-4936-bbdb-2082c4573f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>BookAuthor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1565920317</td>\n",
       "      <td>!%@ (A Nutshell handbook)</td>\n",
       "      <td>Donnalyn Frey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1565920465</td>\n",
       "      <td>!%@ (A Nutshell handbook)</td>\n",
       "      <td>Donnalyn Frey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0133989429</td>\n",
       "      <td>!Arriba! Comunicacion y cultura</td>\n",
       "      <td>Eduardo Zayas-Bazan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>013327974X</td>\n",
       "      <td>!Trato hecho!: Spanish for Real Life, Combined...</td>\n",
       "      <td>John T. McMinn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0452279186</td>\n",
       "      <td>!Yo!</td>\n",
       "      <td>Julia Alvarez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263969</th>\n",
       "      <td>3499232499</td>\n",
       "      <td>Ã?Â?lpiraten.</td>\n",
       "      <td>Janwillem van de Wetering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263970</th>\n",
       "      <td>325721538X</td>\n",
       "      <td>Ã?Â?rger mit Produkt X. Roman.</td>\n",
       "      <td>Joan Aiken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263971</th>\n",
       "      <td>3451274973</td>\n",
       "      <td>Ã?Â?sterlich leben.</td>\n",
       "      <td>Anselm GrÃ?Â¼n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263972</th>\n",
       "      <td>3442725739</td>\n",
       "      <td>Ã?Â?stlich der Berge.</td>\n",
       "      <td>David Guterson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263973</th>\n",
       "      <td>2842192508</td>\n",
       "      <td>Ã?Â?thique en toc</td>\n",
       "      <td>Didier Daeninckx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                          BookTitle  \\\n",
       "0       1565920317                          !%@ (A Nutshell handbook)   \n",
       "1       1565920465                          !%@ (A Nutshell handbook)   \n",
       "2       0133989429                    !Arriba! Comunicacion y cultura   \n",
       "3       013327974X  !Trato hecho!: Spanish for Real Life, Combined...   \n",
       "4       0452279186                                               !Yo!   \n",
       "...            ...                                                ...   \n",
       "263969  3499232499                                      Ã?Â?lpiraten.   \n",
       "263970  325721538X                     Ã?Â?rger mit Produkt X. Roman.   \n",
       "263971  3451274973                                Ã?Â?sterlich leben.   \n",
       "263972  3442725739                              Ã?Â?stlich der Berge.   \n",
       "263973  2842192508                                  Ã?Â?thique en toc   \n",
       "\n",
       "                       BookAuthor  \n",
       "0                   Donnalyn Frey  \n",
       "1                   Donnalyn Frey  \n",
       "2             Eduardo Zayas-Bazan  \n",
       "3                  John T. McMinn  \n",
       "4                   Julia Alvarez  \n",
       "...                           ...  \n",
       "263969  Janwillem van de Wetering  \n",
       "263970                 Joan Aiken  \n",
       "263971             Anselm GrÃ?Â¼n  \n",
       "263972             David Guterson  \n",
       "263973           Didier Daeninckx  \n",
       "\n",
       "[263974 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df = pd.read_csv(books_fpath, dtype={\n",
    "    \"ISBN\": str, \n",
    "    \"BookTitle\": str, \n",
    "    \"BookAuthor\": str, \n",
    "    \"YearOfPublication\": int, \n",
    "    \"Publisher\": str, \n",
    "    \"ImageURLSmall\": str, \n",
    "    \"ImageURLMedium\": str, \n",
    "    \"ImageURLLarge\": str\n",
    "})\n",
    "books_df = books_df.drop([\"Publisher\", \"ImageURLSmall\", \"ImageURLMedium\", \"ImageURLLarge\", \"YearOfPublication\"], axis=1)\n",
    "books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e54e-bce3-4b29-8a99-0c4908032156",
   "metadata": {},
   "source": [
    "<a id=\"Code-Proximity-Search-Features\"></a>\n",
    "### Proximity Search Features\n",
    "Now we create the features that will be used to discover relationships between books. For example, we will have a map from ISBN (book ID) to the users that rated that book and another map from users to the books they rated. This way we don't have to explicitly build a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fcc2f3-af11-4461-9061-ed941b56a875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We created a map of 80043 users to the books they rated. The number of ratings per user:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    80043.000000\n",
       "mean        11.300913\n",
       "std         90.128380\n",
       "min          1.000000\n",
       "50%          1.000000\n",
       "60%          2.000000\n",
       "75%          4.000000\n",
       "90%         13.000000\n",
       "95%         31.000000\n",
       "99%        180.000000\n",
       "99.5%      320.000000\n",
       "max      10267.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_to_books: Dict[str, pd.DataFrame] = {}\n",
    "for user_id, user_df in ratings_df.groupby(\"UserID\"):\n",
    "    user_to_books[user_id] = user_df\n",
    "print(f\"We created a map of {len(user_to_books)} users to the books they rated. The number of ratings per user:\")\n",
    "\n",
    "pd.Series([len(x) for x in user_to_books.values()]).describe(percentiles=[0.6, 0.75, 0.9, 0.95, 0.99, 0.995])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6645f1b-65d5-477f-b53c-2391621276a9",
   "metadata": {},
   "source": [
    "As we can see, the top 1% of users have more than 200 rated books. The user with the most ratings has 10k. If we keep those, the network of books will connect to them too often and be biased towards books they liked. Because of this we sample 200 books from each user that has more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440f115f-e15d-49e2-be8f-5788eed18ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ratings per user after sampling:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    80043.000000\n",
       "mean         7.816599\n",
       "std         24.756617\n",
       "min          1.000000\n",
       "50%          1.000000\n",
       "60%          2.000000\n",
       "75%          4.000000\n",
       "90%         13.000000\n",
       "95%         31.000000\n",
       "99%        180.000000\n",
       "max        200.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "user_to_books: Dict[str, pd.DataFrame] = {\n",
    "    uid: (isbns if len(isbns) <= 200 else isbns.sample(n=200)) \n",
    "    for uid, isbns in user_to_books.items()\n",
    "}\n",
    "print(f\"The number of ratings per user after sampling:\")\n",
    "pd.Series([len(x) for x in user_to_books.values()]).describe(percentiles=[0.6, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2a35af-7c33-476c-afd8-a75b659e92b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We created a map of 225388 books to the users that have rated them. The number of users per book:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    225388.000000\n",
       "mean          4.013341\n",
       "std          15.259392\n",
       "min           1.000000\n",
       "50%           1.000000\n",
       "60%           2.000000\n",
       "75%           3.000000\n",
       "90%           7.000000\n",
       "95%          12.000000\n",
       "99%          46.000000\n",
       "99.5%        75.000000\n",
       "max        2212.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_to_users: Dict[str, pd.DataFrame] = {}\n",
    "for isbn, isbn_df in ratings_df.groupby(\"ISBN\"):\n",
    "    book_to_users[isbn] = isbn_df\n",
    "print(f\"We created a map of {len(book_to_users)} books to the users that have rated them. The number of users per book:\")\n",
    "\n",
    "pd.Series([len(x) for x in book_to_users.values()]).describe(percentiles=[0.6, 0.75, 0.9, 0.95, 0.99, 0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bc3f09-44ac-4444-a7b1-1ce7022584f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 263974 books and 95053 authors. We created two maps: one from books to authors, and one from authors to books.\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_and_spaces(text: str) -> str:\n",
    "    return re.sub('\\W+','', text).lower().strip()\n",
    "books_df[\"FixedAuthor\"] = books_df[\"BookAuthor\"].apply(remove_punctuation_and_spaces)\n",
    "\n",
    "isbns = books_df.ISBN.tolist()\n",
    "authors = [auth.lower() for auth in books_df.FixedAuthor]\n",
    "book_to_author: Dict[str, str] = dict(zip(isbns, authors))\n",
    "book_to_title: Dict[str, str] = dict(zip(isbns, books_df.BookTitle))\n",
    "author_to_books: Dict[str, List[str]] = {}\n",
    "\n",
    "for author, author_df in books_df.groupby(\"FixedAuthor\"):\n",
    "    author = author.lower()\n",
    "    author_to_books[author] = author_df.ISBN.tolist()\n",
    "    \n",
    "print(f\"We have {len(book_to_author)} books and {len(author_to_books)} authors. We created two maps: one from books to authors, and one from authors to books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938c335-45e7-401d-beda-65b5af338ea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"Code-Proximity-Search-Algorithm\"></a>\n",
    "### Proximity Search Algorithm\n",
    "\n",
    "In order to perform proximity search we need two main functions, which are written in mutual recursion:\n",
    "1. **_get_proximal_isbns_user**\n",
    "    * Returns books that are proximal to a given user. \n",
    "    * Will recursively call \"_get_proximal_isbns_book\" to get proximal books from books that this user liked.\n",
    "2. **_get_proximal_isbns_book**\n",
    "    * Returns books that are proximal to a given book. \n",
    "    * Will recursively call \"_get_proximal_isbns_user\" to get proximal books from users that have liked this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a407ea-31c9-4dde-9795-00a9829850ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting proximal books for \"The Selfish Gene\" by \"richarddawkins\"\n"
     ]
    }
   ],
   "source": [
    "def _select_books(books: List[str], max_books: int, already_selected_isbns: Set[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Select a maximum number of books from a list. Chooses the first \"max_books\"\n",
    "    books that haven't previously been selected and are in the target subset\n",
    "    so they can be scored using the factorization machine model.\n",
    "    \n",
    "    Args:\n",
    "        books: A list of books IDs (ISBNs) from which to select.\n",
    "        max_books: The maximum number of books to select (may not be the first\n",
    "            ones if they don't pass the criteria).\n",
    "        already_selected_isbns: A set of books that have already been selected \n",
    "            and which should not be selected again to prevent duplication.        \n",
    "        \n",
    "    Returns:\n",
    "        A list of the selected ISBNs (book IDs).\n",
    "    \"\"\"\n",
    "    selected_books_count = 0\n",
    "    selected_books: List[str] = []\n",
    "    \n",
    "    for isbn in books:\n",
    "        if isbn in already_selected_isbns:\n",
    "            continue\n",
    "            \n",
    "        if selected_books_count >= max_books:\n",
    "            break\n",
    "            \n",
    "        already_selected_isbns.add(isbn)\n",
    "        \n",
    "        # only add ISBNs for which we can predict the rating using the \n",
    "        # factorization machine model (those that are in the target subset)\n",
    "        if isbn in target_isbns:\n",
    "            selected_books.append(isbn)\n",
    "            selected_books_count += 1\n",
    "            \n",
    "    return selected_books\n",
    "\n",
    "\n",
    "def _get_proximal_isbns_user(\n",
    "    user_id: str, \n",
    "    n_results: int, \n",
    "    book_to_users: Dict[str, pd.DataFrame], \n",
    "    user_to_books: Dict[str, pd.DataFrame],\n",
    "    book_to_author: Dict[str, str],\n",
    "    author_to_books: Dict[str, List[str]],\n",
    "    visited_isbns: Set[str],\n",
    "    visited_users: Set[str],\n",
    "    already_selected_isbns: Set[str],\n",
    "    min_like_rating: int = 7,\n",
    "    max_books_same_user: int = 8,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get proximal books for a given user, which will contain books that they\n",
    "    liked directly, or \"liked\" books of users that liked the same books as the\n",
    "    current user.\n",
    "    \n",
    "    Args:\n",
    "        user_id: The unique identifier of the user for which we are searching\n",
    "            proximal books.\n",
    "        n_results: The number of books to be returned.\n",
    "        book_to_users: A map from books to the users that rated them.\n",
    "        user_to_books: A map from users to the books they rated.\n",
    "        book_to_author: A map from books to their author.\n",
    "        author_to_books: A map from authors to the books they wrote.\n",
    "        visited_isbns: A set of books (ISBNs) visited during the search.\n",
    "        visited_users: A set of users (user IDs) visited during the search.\n",
    "        already_selected_isbns: A set of books that were already selected to\n",
    "            be returned (to prevent them from being added twice).\n",
    "        min_like_rating: The minimum rating (out of 10) to consider a book as\n",
    "            \"liked\" by the user. The search will only continue with books that\n",
    "            a user has liked.\n",
    "        max_books_same_user: The maximum number of books that will come \n",
    "            directly from this user's liked books. The rest will come from\n",
    "            what similar users liked.\n",
    "    \n",
    "    Returns:\n",
    "        A list of ISBNs of books that are considered proximal to the user.\n",
    "    \"\"\"\n",
    "    visited_users.add(user_id)\n",
    "    \n",
    "    if n_results <= 0:\n",
    "        return []\n",
    "    \n",
    "    proximal_isbns: List[str] = []\n",
    "\n",
    "    # dataframe of books that this user has rated, sorted by the rating descending\n",
    "    books_df = user_to_books[user_id].sort_values(by=\"BookRating\", ascending=False)\n",
    "    # eliminate books that have already been visited\n",
    "    books_df = books_df[~books_df.ISBN.isin(visited_isbns)]\n",
    "    \n",
    "    # books that the person has \"liked\" given the rating threshold\n",
    "    liked_books = books_df[books_df.BookRating > min_like_rating]\n",
    "    # sort by rating descending to iterate through the most liked books first\n",
    "    liked_books = liked_books.sort_values(by=\"BookRating\", ascending=False)\n",
    "    \n",
    "    # n_remaining represents the number of books that still need to be found\n",
    "    # out of the maximum n_results that this function needs to return\n",
    "    n_remaining = n_results\n",
    "    \n",
    "    # select at most min(n_remaining, max_books_same_user) of the liked books \n",
    "    # of this user, starting with the highest rated ones\n",
    "    liked_books = liked_books.ISBN.tolist()\n",
    "    selected_books = _select_books(\n",
    "        books=liked_books,\n",
    "        max_books=min(n_remaining, max_books_same_user),\n",
    "        already_selected_isbns=already_selected_isbns,\n",
    "    )\n",
    "    proximal_isbns.extend(selected_books)\n",
    "    n_remaining -= len(selected_books)\n",
    "    \n",
    "    # weights assigned to each book; based on this number we will extract a\n",
    "    # proportional number of proximal books from each of them; high ratings\n",
    "    # get a much larger proportion since the weights are the ratings squared\n",
    "    weights = [1 + rating * rating for rating in books_df.BookRating]\n",
    "    for book_index, book_id in enumerate(books_df.ISBN):\n",
    "        if n_remaining <= 0:\n",
    "            break\n",
    "        \n",
    "        # extract a proportional number of proximal books from this book\n",
    "        book_n_results = int(1 + weights[book_index] / sum(weights[book_index:]) * n_remaining)\n",
    "        book_results = _get_proximal_isbns_book(\n",
    "            isbn=book_id,\n",
    "            n_results=book_n_results,\n",
    "            book_to_users=book_to_users,\n",
    "            user_to_books=user_to_books,\n",
    "            book_to_author=book_to_author,\n",
    "            author_to_books=author_to_books,\n",
    "            visited_isbns=visited_isbns,\n",
    "            visited_users=visited_users,\n",
    "            already_selected_isbns=already_selected_isbns,\n",
    "        )\n",
    "            \n",
    "        proximal_isbns.extend(book_results[:n_remaining])\n",
    "        n_remaining = n_results - len(proximal_isbns)\n",
    "        \n",
    "    return proximal_isbns    \n",
    "    \n",
    "    \n",
    "def _get_proximal_isbns_book(\n",
    "    isbn: str, \n",
    "    n_results: int, \n",
    "    book_to_users: Dict[str, pd.DataFrame], \n",
    "    user_to_books: Dict[str, pd.DataFrame],\n",
    "    book_to_author: Dict[str, str],\n",
    "    author_to_books: Dict[str, List[str]],\n",
    "    visited_isbns: Set[str],\n",
    "    visited_users: Set[str],\n",
    "    already_selected_isbns: Set[str],\n",
    "    max_books_same_author: int = 10,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get proximal books for a given book, which will contain books from the same\n",
    "    author, or books from users that liked this book.\n",
    "    \n",
    "    Args:\n",
    "        isbn: The unique identifier of the book for which we are searching\n",
    "            proximal books.\n",
    "        n_results: The number of books to be returned.\n",
    "        book_to_users: A map from books to the users that rated them.\n",
    "        user_to_books: A map from users to the books they rated.\n",
    "        book_to_author: A map from books to their author.\n",
    "        author_to_books: A map from authors to the books they wrote.\n",
    "        visited_isbns: A set of books (ISBNs) visited during the search.\n",
    "        visited_users: A set of users (user IDs) visited during the search.\n",
    "        already_selected_isbns: A set of books that were already selected to\n",
    "            be returned (to prevent them from being added twice).\n",
    "        min_like_rating: The minimum rating (out of 10) to consider a book as\n",
    "            \"liked\" by the user. The search will only continue with books that\n",
    "            a user has liked.\n",
    "        max_books_same_author: The maximum number of books from the same author\n",
    "            that will be included directly in the first iteration. More books \n",
    "            from this author can be included, but they would have to be \n",
    "            discovered from similar users in the recursive call of \n",
    "            \"_get_proximal_isbns_user\".\n",
    "    \n",
    "    Returns:\n",
    "        A list of ISBNs of books that are considered proximal to the given book.\n",
    "    \"\"\"\n",
    "    visited_isbns.add(isbn)\n",
    "    \n",
    "    if n_results <= 0:\n",
    "        return []\n",
    "    \n",
    "    proximal_isbns: List[str] = []\n",
    "    \n",
    "    # n_remaining represents the number of books that still need to be found\n",
    "    # out of the maximum n_results that this function needs to return\n",
    "    n_remaining = n_results\n",
    "    \n",
    "    # select at random at most min(n_remaining, max_books_same_author) other\n",
    "    # books of this author\n",
    "    same_author_books = author_to_books[book_to_author[isbn]]\n",
    "    same_author_books = random.sample(same_author_books, len(same_author_books))\n",
    "    selected_books = _select_books(\n",
    "        books=same_author_books,\n",
    "        max_books=min(n_remaining, max_books_same_author),\n",
    "        already_selected_isbns=already_selected_isbns,\n",
    "    )\n",
    "    proximal_isbns.extend(selected_books)\n",
    "    n_remaining -= len(selected_books)\n",
    "    \n",
    "    # dataframe of users that have rated this book, sorted by the rating descending\n",
    "    users_df = book_to_users[isbn].sort_values(by=\"BookRating\", ascending=False, ignore_index=True)\n",
    "    # removing the users that have already been visited\n",
    "    users_df = users_df[~users_df.UserID.isin(visited_users)]\n",
    "    \n",
    "    \n",
    "    # weights assigned to each user; based on this number we will extract a\n",
    "    # proportional number of proximal books from each of them; high ratings\n",
    "    # get a much larger proportion since the weights are the ratings squared\n",
    "    weights = [1 + rating * rating for rating in users_df.BookRating]\n",
    "    \n",
    "    for user_index, user_id in enumerate(users_df.UserID):\n",
    "        if n_remaining <= 0:\n",
    "            break\n",
    "        \n",
    "        # extract a proportional number of proximal books from this user\n",
    "        user_n_results = int(1 + weights[user_index] / sum(weights[user_index:]) * n_remaining)\n",
    "        user_results = _get_proximal_isbns_user(\n",
    "            user_id=user_id,\n",
    "            n_results=user_n_results,\n",
    "            book_to_users=book_to_users, \n",
    "            user_to_books=user_to_books,\n",
    "            book_to_author=book_to_author,\n",
    "            author_to_books=author_to_books,\n",
    "            visited_isbns=visited_isbns,\n",
    "            visited_users=visited_users,\n",
    "            already_selected_isbns=already_selected_isbns,\n",
    "        )\n",
    "        \n",
    "        proximal_isbns.extend(user_results[:n_remaining])\n",
    "        n_remaining = n_results - len(proximal_isbns)\n",
    "        \n",
    "    return proximal_isbns    \n",
    "    \n",
    "\n",
    "def get_proximal_isbns(\n",
    "    isbn: str, \n",
    "    n_results: int, \n",
    "    book_to_users: Dict[str, pd.DataFrame], \n",
    "    user_to_books: Dict[str, pd.DataFrame],\n",
    "    book_to_author: Dict[str, str],\n",
    "    author_to_books: Dict[str, List[str]],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get proximal books for a given book, which will contain books from the same\n",
    "    author, or books from users that liked this book.\n",
    "    \n",
    "    Args:\n",
    "        isbn: The unique identifier of the book for which we are searching\n",
    "            proximal books.\n",
    "        n_results: The number of books to be returned.\n",
    "        book_to_users: A map from books to the users that rated them.\n",
    "        user_to_books: A map from users to the books they rated.\n",
    "        book_to_author: A map from books to their author.\n",
    "        author_to_books: A map from authors to the books they wrote.\n",
    "    \n",
    "    Returns:\n",
    "        A list of ISBNs of books that are considered proximal to the given book.\n",
    "    \"\"\"\n",
    "        \n",
    "    random.seed(42)\n",
    "    \n",
    "    already_selected_isbns: Set[str] = set([isbn])\n",
    "    visited_isbns: Set[str] = set()\n",
    "    visited_users: Set[str] = set()\n",
    "    \n",
    "    return _get_proximal_isbns_book(\n",
    "        isbn=isbn, \n",
    "        n_results=n_results, \n",
    "        book_to_users=book_to_users, \n",
    "        user_to_books=user_to_books,\n",
    "        book_to_author=book_to_author,\n",
    "        author_to_books=author_to_books,\n",
    "        visited_isbns=visited_isbns,\n",
    "        visited_users=visited_users,\n",
    "        already_selected_isbns=already_selected_isbns,\n",
    "    )\n",
    "\n",
    "def map_if_duplicate(isbn: str, same_book_isbn_map: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Map current ISBN to the latest version if it is a duplicate.\n",
    "    \n",
    "    Args:\n",
    "        isbn: The ISBN to be mapped if it is a duplicate.\n",
    "        same_book_isbn_map: A map from duplicate ISBN's (old version of a book)\n",
    "            to the latest ISBN (the latest version of that book).\n",
    "            \n",
    "    Returns:\n",
    "        The ISBN after being mapped or not.\n",
    "    \"\"\"\n",
    "    if isbn in same_book_isbn_map:\n",
    "        return same_book_isbn_map[isbn]\n",
    "    return isbn\n",
    "\n",
    "# book_isbn = \"0553381695\"\n",
    "book_isbn = \"0192177737\"\n",
    "book_isbn = map_if_duplicate(book_isbn, same_book_isbn_map)\n",
    "\n",
    "proximal_isbns = get_proximal_isbns(book_isbn, 100, book_to_users, user_to_books, book_to_author, author_to_books)\n",
    "print(f'Getting proximal books for \"{book_to_title[book_isbn]}\" by \"{book_to_author[book_isbn]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f76c6a3-e47d-47f2-bbf7-e0854f2c38a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_isbn in proximal_isbns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f85aeed-a9b1-4878-be18-ac4ed09c9ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'richarddawkins',\n",
       " 'byronpreiss',\n",
       " 'byrdbaggett',\n",
       " 'billphillips',\n",
       " 'howardmshapiro',\n",
       " 'panjajurgens',\n",
       " 'gregpahl',\n",
       " 'roberthendrickson',\n",
       " 'byronpreiss',\n",
       " 'byronpreiss',\n",
       " 'byrdbaggett']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book_to_author[x] for x in proximal_isbns][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787d5425-409d-4d4c-bcdb-3d0237fd1886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Der entzauberte Regenbogen. Wissenschaft, Aberglaube und die Kraft der Phantasie.',\n",
       " \"A Devil's Chaplain: Reflections on Hope, Lies, Science and Love\",\n",
       " 'The Extended Phenotype: The Long Reach of the Gene (Popular Science)',\n",
       " \"God's Utility Function (Phoenix 60p Paperbacks)\",\n",
       " 'Unweaving the Rainbow: Science, Delusion and the Appetite for Wonder',\n",
       " 'Climbing Mount Improbable',\n",
       " 'The Blind Watchmaker: Why the Evidence of Evolution Reveals a Universe Without Design',\n",
       " 'The Extended Phenotype: The Long Reach of the Gene',\n",
       " 'River Out of Eden: A Darwinian View of Life (Science Masters)',\n",
       " 'River Out of Eden: A Darwinian View of Life (Science Masters Series)',\n",
       " 'The Microverse',\n",
       " 'The Pocket Power Book Of Leadership',\n",
       " 'Body for Life: 12 Weeks to Mental and Physical Strength',\n",
       " \"Dr. Shapiro's Picture Perfect Weight Loss : The Visual Program for Permanent Weight Loss\",\n",
       " 'They Call Themselves Queens: The Transformation Series',\n",
       " \"Complete Idiot's Guide to Saving the Environment\",\n",
       " 'The Facts on File Encyclopedia of Word and Phrase Origins (Facts on File)',\n",
       " \"Best Children's Books in the World\",\n",
       " 'The Ultimate Zombie',\n",
       " 'The Book Of Excellence : 236 Habits of Successful Salespeople']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book_to_title[x] for x in proximal_isbns][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a8adc3-66ec-4c66-b8e9-6c03e5cea1d7",
   "metadata": {},
   "source": [
    "<a id=\"Code-Parallel-Proximity-Search\"></a>\n",
    "### Parallel Proximity Search\n",
    "All that is left to do is find the proximal ISBNs for all 170k books in the context subset. The workers require the presence of some very large objects (for example, a dictionary that maps an ISBN to all users that rated that book; another one is a dictionary that maps users to all books they rated, etc.). If we used a functools.partial function to send the parameters to the worker then the large objects would be sent to the workers for each task. Instead we need to use multiprocessing.Pool's initializer to attach the objects to the function object itself, which will send the large dictionaries only once to each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af180384-c8e9-4371-a1f2-fa54a1a47c68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113d522919fc46ef877acce045e3ab4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wrapper_get_proximal_isbns(isbn: str) -> List[str]:\n",
    "    return get_proximal_isbns(\n",
    "        isbn=isbn,\n",
    "        n_results=wrapper_get_proximal_isbns.n_results,\n",
    "        book_to_users=wrapper_get_proximal_isbns.book_to_users, \n",
    "        user_to_books=wrapper_get_proximal_isbns.user_to_books, \n",
    "        book_to_author=wrapper_get_proximal_isbns.book_to_author, \n",
    "        author_to_books=wrapper_get_proximal_isbns.author_to_books,\n",
    "    )\n",
    "\n",
    "def init_worker(function: Callable) -> None:\n",
    "    \"\"\"\n",
    "    Adds the global variables that are needed for the \"get_proximal_isbns\"\n",
    "    function to the wrapper function object that will be pickled and sent to\n",
    "    the worker.\n",
    "    \"\"\"\n",
    "    function.n_results = 100\n",
    "    function.book_to_users = book_to_users\n",
    "    function.user_to_books = user_to_books\n",
    "    function.book_to_author = book_to_author\n",
    "    function.author_to_books = author_to_books\n",
    "\n",
    "isbns: List[str] = list(context_isbns)\n",
    "with Pool(initializer=init_worker, initargs=(wrapper_get_proximal_isbns,)) as p:\n",
    "    proximal_isbns: List[List[str]] = list(\n",
    "        tqdm(\n",
    "            p.imap(wrapper_get_proximal_isbns, isbns), \n",
    "            total=len(isbns)\n",
    "        )\n",
    "    )\n",
    "\n",
    "isbn_to_proximal_isbns: Dict[str, List[str]] = dict(zip(isbns, proximal_isbns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a275a5-14f2-40ed-bb99-fb77ceeac792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(output_fpath, \"w\") as f:\n",
    "    f.write(json.dumps(isbn_to_proximal_isbns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2753445-a14b-494d-a219-940af7c09d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
